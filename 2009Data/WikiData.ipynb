{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering player Wikipedia data\n",
    "\n",
    "## Part 1: Player Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "\n",
    "playerinfo = []\n",
    "with open(\"USWNT/playerdata.csv\", \"r\", newline = \"\") as infile:\n",
    "    csvin = csv.reader(infile)\n",
    "    headers = next(csvin)\n",
    "    for line in csvin:\n",
    "        playerinfo.append(line)\n",
    "\n",
    "filename = headers.index(\"Player Filename\")\n",
    "firstname = headers.index(\"Firstname\")\n",
    "surname = headers.index(\"Surname\")\n",
    "\n",
    "wikidata = []\n",
    "\n",
    "for singleplayer in playerinfo:\n",
    "    wikidatarow = []\n",
    "    playerfilename = singleplayer[filename]\n",
    "    playerfirstname = singleplayer[firstname]\n",
    "    playersurname = singleplayer[surname]\n",
    "\n",
    "    # Make Players' Wikipedia URLs\n",
    "    if playersurname == \"Campbell\" or playersurname == \"Smith\" or playersurname == \"Williams\" or playersurname == \"Fox\":\n",
    "        wikiplayerurl = \"https://en.wikipedia.org/wiki/\" + playerfirstname + \"_\" + playersurname + \"_(soccer)\"\n",
    "    else:\n",
    "        wikiplayerurl = \"https://en.wikipedia.org/wiki/\" + playerfirstname + \"_\" + playersurname\n",
    "    \n",
    "    # Make filenames to write the HTML from the Wikipedia pages\n",
    "    last_slash = wikiplayerurl.rindex(\"/\")\n",
    "    playerwikifilename = \"PlayerWiki\" + wikiplayerurl[last_slash:] + \".html\"\n",
    "    \n",
    "    # Request HTML from Wikipedia\n",
    "    outfile = open(playerwikifilename, \"w\")\n",
    "    result = requests.get(wikiplayerurl)\n",
    "    print(result.text, file=outfile)\n",
    "    outfile.close()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Collect player names and filenames to keep track of what's where\n",
    "    wikidatarow.append(playerfilename)\n",
    "    wikidatarow.append(playerfirstname)\n",
    "    wikidatarow.append(playersurname)\n",
    "    wikidatarow.append(playerwikifilename)\n",
    "    \n",
    "    wikidata.append(wikidatarow)\n",
    "\n",
    "# Write it out to a csv for preservation and accountability\n",
    "wikidataheaders = [\"Player Filename\", \"Firstname\", \"Surname\", \"PlayerWiki Filename\"]\n",
    "\n",
    "outfile = open('PlayerWiki/playerwikidata.csv', 'w')\n",
    "csv_out = csv.writer(outfile)\n",
    "csv_out.writerow(wikidataheaders)\n",
    "csv_out.writerows(wikidata)\n",
    "outfile.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Finding player college teams and the Wikipedia page for the teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "\n",
    "wikidata = []\n",
    "with open('PlayerWiki/playerwikidata.csv', \"r\", newline = \"\") as infile:\n",
    "    csvin = csv.reader(infile)\n",
    "    wikidataheaders = next(csvin)\n",
    "    for line in csvin:\n",
    "        wikidata.append(line)\n",
    "\n",
    "playerwikihtml = wikidataheaders.index(\"PlayerWiki Filename\")\n",
    "\n",
    "for playerwiki in wikidata:\n",
    "    playerwikihtmlfile = playerwiki[playerwikihtml]\n",
    "    infile = open(playerwikihtmlfile, \"r\")\n",
    "    page = infile.read()\n",
    "    tree = html.fromstring(page)\n",
    "    \n",
    "    headers = tree.xpath('//th[contains(@style,\"background-color: #b0c4de\")]') # Identify every header, made distinct by the background color.\n",
    "\n",
    "    counter = 0\n",
    "    for element in headers: # Circle through the headers\n",
    "        if element.text == \"College career\": # Find which header has the college career\n",
    "            collegelocation = headers[counter]\n",
    "            nextheader = headers[counter + 1] # Identify whatever is the next header after college\n",
    "        else:\n",
    "            pass\n",
    "        counter = counter + 1\n",
    "    try:\n",
    "        collegehtml = page[page.index(collegelocation.text):page.index(nextheader.text)] # Only look at the text between the two headers\n",
    "        littletree=html.fromstring(collegehtml)\n",
    "        collegelink = littletree.xpath('//a/@href') # Find the links within the college career section\n",
    "        for link in collegelink:\n",
    "            if link.startswith(\"#\") == True: # Weed out any cite notes or other comments that aren't actual links\n",
    "                pass\n",
    "            else:\n",
    "                playerwiki.append(link[link.rindex('/'):])\n",
    "                \n",
    "    except:\n",
    "        playerwiki.append(\"NoCollege\")\n",
    "        \n",
    "\n",
    "\n",
    "wikidataheaders.append(\"First College Team Link\")\n",
    "wikidataheaders.append(\"Second College Team Link\")\n",
    "outfile = open('Collegeteam/collegeteamlinks.csv', 'w')\n",
    "csv_out = csv.writer(outfile)\n",
    "csv_out.writerow(wikidataheaders)\n",
    "csv_out.writerows(wikidata)\n",
    "outfile.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "wikidata = []\n",
    "with open('Collegeteam/collegeteamlinks.csv', 'r', newline = \"\") as infile:\n",
    "    csvin = csv.reader(infile)\n",
    "    wikidataheaders = next(csvin)\n",
    "    for line in csvin:       \n",
    "        wikidata.append(line)\n",
    "        \n",
    "firstcollegeteam = wikidataheaders.index(\"First College Team Link\") # Index first and second college teams, since some players attended two schools\n",
    "secondcollegeteam = wikidataheaders.index(\"Second College Team Link\")\n",
    "\n",
    "for playerwiki in wikidata:\n",
    "    try:\n",
    "        secondcollege = playerwiki[secondcollegeteam] # If there's an indexing error, it means there isn't a second college, so add in a null value\n",
    "    except:\n",
    "        playerwiki.append(\"N/A\")\n",
    "    \n",
    "    firstcollege = playerwiki[firstcollegeteam]\n",
    "    secondcollege = playerwiki[secondcollegeteam]\n",
    "    \n",
    "    if firstcollege == \"NoCollege\": # Don't try to do anything if the player didn't go to college\n",
    "        playerwiki.append(\"N/A\")\n",
    "    else:\n",
    "        firstlinkbuild = \"https://en.wikipedia.org/wiki\" + firstcollege\n",
    "        outfile = open('Collegeteam' + firstcollege + '.html', 'w')\n",
    "        \n",
    "        result = requests.get(firstlinkbuild) # Request the college team page from wikipedia\n",
    "        print(result.text, file=outfile)\n",
    "        time.sleep(2)\n",
    "        outfile.close()\n",
    "    \n",
    "    if secondcollege == \"N/A\":\n",
    "        pass\n",
    "    else:\n",
    "        secondlinkbuild = \"https://en.wikipedia.org/wiki\" + secondcollege # If a player went to a second college, read in the page for their second team\n",
    "        outfile = open('Collegeteam' + secondcollege + '.html', 'w')\n",
    "        \n",
    "        result = requests.get(secondlinkbuild)\n",
    "        print(result.text, file=outfile)\n",
    "        time.sleep(2)\n",
    "        outfile.close()\n",
    "    \n",
    "outfile = open('Collegeteam/collegeteamlinks.csv', 'w')\n",
    "csv_out = csv.writer(outfile)\n",
    "csv_out.writerow(wikidataheaders)\n",
    "csv_out.writerows(wikidata)\n",
    "outfile.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Finding the Wiki page for the actual university"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "\n",
    "wikidata = []\n",
    "with open('Collegeteam/collegeteamlinks.csv', 'r', newline = \"\") as infile:\n",
    "    csvin = csv.reader(infile)\n",
    "    wikidataheaders = next(csvin)\n",
    "    for line in csvin:       \n",
    "        wikidata.append(line)\n",
    "\n",
    "firstcollegeteam = wikidataheaders.index(\"First College Team Link\")\n",
    "secondcollegeteam = wikidataheaders.index(\"Second College Team Link\")\n",
    "\n",
    "wikidataheaders.append(\"First College\")\n",
    "wikidataheaders.append(\"Second College\")\n",
    "\n",
    "for playerwiki in wikidata:\n",
    "    \n",
    "    firstcollege = playerwiki[firstcollegeteam]\n",
    "    secondcollege = playerwiki[secondcollegeteam]        \n",
    "        \n",
    "    if firstcollege == \"NoCollege\":\n",
    "        playerwiki.append(\"N/A\")\n",
    "    else:\n",
    "        infile = open('Collegeteam' + firstcollege + \".html\", 'rb')\n",
    "        page = infile.read()\n",
    "        tree=html.fromstring(page)    \n",
    "        college = tree.xpath('//tr[contains(th,\"University\")]/td/a/@href') # Identify the link to the university's page\n",
    "        if college == []:\n",
    "            playerwiki.append('Collegewiki' + firstcollege)\n",
    "        else:\n",
    "            for link in college:\n",
    "                collegelink = 'Collegewiki' + link[link.rindex('/'):] # Removes the /wiki from the beginning of the url\n",
    "                playerwiki.append(collegelink)\n",
    "        \n",
    "    if secondcollege == \"N/A\":\n",
    "        playerwiki.append(\"N/A\")\n",
    "    else:\n",
    "        infile = open('Collegeteam' + secondcollege + \".html\", 'rb')\n",
    "        page = infile.read()\n",
    "        tree=html.fromstring(page)    \n",
    "        college = tree.xpath('//tr[contains(th,\"University\")]/td/a/@href') # Identify the link to the second university's page\n",
    "        for link in college:\n",
    "            collegelink = 'Collegewiki' + link[link.rindex('/'):] # Removes the /wiki from the beginning of the url\n",
    "            playerwiki.append(collegelink)\n",
    "        \n",
    "\n",
    "outfile = open('Collegewiki/collegelinks.csv', 'w')\n",
    "csv_out = csv.writer(outfile)\n",
    "csv_out.writerow(wikidataheaders)\n",
    "csv_out.writerows(wikidata)\n",
    "outfile.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata = []\n",
    "with open(\"Collegewiki/collegelinks.csv\", \"r\", newline = \"\") as infile:\n",
    "    csvin = csv.reader(infile)\n",
    "    wikidataheaders = next(csvin)\n",
    "    for line in csvin:\n",
    "        wikidata.append(line)\n",
    "\n",
    "firstcollege = wikidataheaders.index(\"First College\")\n",
    "secondcollege = wikidataheaders.index(\"Second College\")\n",
    "\n",
    "for playerwiki in wikidata:\n",
    "    firstcollegelink = playerwiki[firstcollege]\n",
    "    secondcollegelink = playerwiki[secondcollege]\n",
    "    \n",
    "    if firstcollegelink == \"N/A\": # Read in the first college's page, if applicable\n",
    "        pass\n",
    "    else:\n",
    "        firstcollegeurl = 'https://wikipedia.org/wiki' + firstcollegelink[firstcollegelink.rindex('/'):]\n",
    "        \n",
    "        outfile = open(firstcollegelink + '.html', 'w')\n",
    "        result = requests.get(firstcollegeurl)\n",
    "        print(result.text, file=outfile)\n",
    "        time.sleep(2)\n",
    "        outfile.close()\n",
    "    \n",
    "    if secondcollegelink == \"N/A\": # Read in the second college's page, if applicable\n",
    "        pass\n",
    "    else:\n",
    "        secondcollegeurl = 'https://wikipedia.org/wiki' + secondcollegelink[secondcollegelink.rindex('/'):]\n",
    "        \n",
    "        outfile = open(secondcollegelink + '.html', 'w')\n",
    "        result = requests.get(secondcollegeurl)\n",
    "        print(result.text, file=outfile)\n",
    "        time.sleep(2)\n",
    "        outfile.close()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata = []\n",
    "with open(\"Collegewiki/collegelinks.csv\", \"r\", newline = \"\") as infile:\n",
    "    csvin = csv.reader(infile)\n",
    "    wikidataheaders = next(csvin)\n",
    "    for line in csvin:\n",
    "        wikidata.append(line)\n",
    "\n",
    "firstcollege = wikidataheaders.index(\"First College\")\n",
    "secondcollege = wikidataheaders.index(\"Second College\")\n",
    "\n",
    "wikidataheaders.append(\"First College Name\") # Adding headers to the csv for all the data I'm going to collect\n",
    "wikidataheaders.append(\"First College Public\")\n",
    "wikidataheaders.append(\"First College Private\")\n",
    "wikidataheaders.append(\"First College Community\")\n",
    "wikidataheaders.append(\"First College Location\")\n",
    "wikidataheaders.append(\"First College Enrollment\")\n",
    "\n",
    "wikidataheaders.append(\"Second College Name\")\n",
    "wikidataheaders.append(\"Second College Public\")\n",
    "wikidataheaders.append(\"Second College Private\")\n",
    "wikidataheaders.append(\"Second College Community\")\n",
    "wikidataheaders.append(\"Second College Location\")\n",
    "wikidataheaders.append(\"Second College Enrollment\")\n",
    "\n",
    "\n",
    "for playerwiki in wikidata:\n",
    "    firstcollegelink = playerwiki[firstcollege]\n",
    "    secondcollegelink = playerwiki[secondcollege]\n",
    "    \n",
    "    if firstcollegelink == \"N/A\": # Add null values to fill the space that won't have data to fill\n",
    "        for loop in range(5):\n",
    "            playerwiki.append(\"N/A\")\n",
    "\n",
    "    else:    \n",
    "        infile = open(firstcollegelink + '.html', 'rb') \n",
    "        page = infile.read()\n",
    "        tree=html.fromstring(page)    \n",
    "    \n",
    "        schoolname = tree.xpath('//h1/text()') # Identify the name of the school\n",
    "        \n",
    "        if tree.xpath('//tr[contains(th,\"Type\")]/td/a[contains(text(),\"Public\")]/text()') == []: # Is it a public university?\n",
    "            publiccollege = 'n'\n",
    "        else:\n",
    "            publiccollege = 'y'\n",
    "        \n",
    "        if tree.xpath('//tr[contains(th,\"Type\")]/td/a[contains(text(),\"Private\")]/text()') == []: # Is it a private university?\n",
    "            privatecollege = 'n'\n",
    "        else:\n",
    "            privatecollege = 'y'\n",
    "            \n",
    "        if tree.xpath('//tr[contains(th,\"Type\")]/td/a[contains(text(),\"Community\")]/text()') == []: # Is it a community college?\n",
    "            communitycollege = 'n'\n",
    "        else:\n",
    "            communitycollege = 'y'\n",
    "            \n",
    "        students = tree.xpath('//tr[contains(th,\"Students\")]/td/text()') # Identify the number of students\n",
    "        location = tree.xpath('//tr[contains(th,\"Location\")]/td/div/a/text()') # Identify the location\n",
    "    \n",
    "        playerwiki.append(\",\".join(schoolname))\n",
    "        playerwiki.append(publiccollege)\n",
    "        playerwiki.append(privatecollege)\n",
    "        playerwiki.append(communitycollege)\n",
    "        playerwiki.append(\",\".join(location))\n",
    "        playerwiki.append(\",\".join(students))\n",
    "\n",
    "\n",
    "    if secondcollegelink == \"N/A\": # Add null values for anyone who didn't go to a second college\n",
    "        for loop in range(5):\n",
    "            playerwiki.append(\"N/A\")\n",
    "\n",
    "    else:    \n",
    "        infile = open(secondcollegelink + '.html', 'rb')\n",
    "        page = infile.read()\n",
    "        tree=html.fromstring(page)    \n",
    "    \n",
    "        schoolname = tree.xpath('//h1/text()') # Identify the name of the second school\n",
    "        \n",
    "        if tree.xpath('//tr[contains(th,\"Type\")]/td/a[contains(text(),\"Public\")]/text()') == []: # Is it a public university?\n",
    "            publiccollege = 'n'\n",
    "        else:\n",
    "            publiccollege = 'y'\n",
    "        \n",
    "        if tree.xpath('//tr[contains(th,\"Type\")]/td/a[contains(text(),\"Private\")]/text()') == []: # Is it a private university?\n",
    "            privatecollege = 'n'\n",
    "        else:\n",
    "            privatecollege = 'y'\n",
    "            \n",
    "        if tree.xpath('//tr[contains(th,\"Type\")]/td/a[contains(text(),\"Community\")]/text()') == []: # Is it a community college?\n",
    "            communitycollege = 'n'\n",
    "        else:\n",
    "            communitycollege = 'y'\n",
    "            \n",
    "        students = tree.xpath('//tr[contains(th,\"Students\")]/td/text()') # Identify the number of students\n",
    "        location = tree.xpath('//tr[contains(th,\"Location\")]/td/div/a/text()') # Identify the location\n",
    "    \n",
    "        playerwiki.append(\",\".join(schoolname))\n",
    "        playerwiki.append(publiccollege)\n",
    "        playerwiki.append(privatecollege)\n",
    "        playerwiki.append(communitycollege)\n",
    "\n",
    "        playerwiki.append(\",\".join(location))\n",
    "        playerwiki.append(\",\".join(students))\n",
    "\n",
    "\n",
    "outfile = open('Collegewiki/collegedata.csv', 'w')\n",
    "csv_out = csv.writer(outfile)\n",
    "csv_out.writerow(wikidataheaders)\n",
    "csv_out.writerows(wikidata)\n",
    "outfile.close()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation for this dataset can be found at:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
